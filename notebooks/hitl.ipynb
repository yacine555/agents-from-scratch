{
 "cells": [
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "f5b66299",
=======
   "id": "dc6a2ba0",
>>>>>>> main
   "metadata": {},
   "source": [
    "# Human-in-the-Loop\n",
    "\n",
    "We've tested two different email assistant, both of which can triage emails and use tools to respond to them. But do we actually *trust* them to manage our inbox? Few would trust an AI to manage their inbox without some human oversight immediately, which is why human-in-the-loop (HITL) is a critical pattern for many agent systems.\n",
    "\n",
    "![overview-img](img/overview_hitl.png)\n",
    "\n",
    "## LangGraph Interrupts\n",
    "\n",
    "The HITL (Human-In-The-Loop) pattern is useful for applications where decisions require human validation. LangGraph provides built-in support for this through its [interrupt mechanism](https://langchain-ai.github.io/langgraph/concepts/interrupts/), allowing us to pause execution of an agent and request human input when needed. Let's add HITL to our email assistant after specific tools are called.\n",
    "\n",
    "### Simple Interrupt Example\n",
    "\n",
    "First, let's just show a simple example for how to use the `interrupt` function. Assume we want a simple agent that can ask the user a question and then use that information. The `interrupt` function can be used for this purpose:\n",
    "\n",
    "```\n",
    "location = interrupt(ask.question)\n",
    "```\n",
    "\n",
    "When this line executes:\n",
    "1. It raises a `GraphInterrupt` exception, which pauses the graph execution\n",
    "2. It surfaces the value passed in (`ask.question`) to the client TODO: explain the client\n",
    "3. Execution stops at this point until resumed \n",
    "4. When resumed, the function returns the value provided by the human\n",
    "\n",
    "Here's a minimal example of how to implement this using `interrupt`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "fe8c14c9",
=======
   "id": "09b68a88",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langgraph.graph import MessagesState, START, END, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import Command, interrupt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"Call to surf the web.\"\"\"\n",
    "    return f\"I looked up: {query}. Result: It's sunny in San Francisco.\"\n",
    "\n",
    "# We can define a tool definition for `ask_human`\n",
    "class AskHuman(BaseModel):\n",
    "    \"\"\"Ask the human a question\"\"\"\n",
    "    question: str\n",
    "\n",
    "tools = [search, AskHuman]\n",
    "tool_node = ToolNode([search])\n",
    "\n",
    "# Set up the model\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"openai:gpt-4o\", temperature=0.0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return END\n",
    "    # If tool call is asking Human, we return that node\n",
    "    elif last_message.tool_calls[0][\"name\"] == \"AskHuman\":\n",
    "        return \"ask_human\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"action\"\n",
    "\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    message = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "def ask_human(state):\n",
    "    # Get the tool call ID \n",
    "    tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "    # Get the AskHuman schema\n",
    "    ask = AskHuman.model_validate(state[\"messages\"][-1].tool_calls[0][\"args\"])\n",
    "    # Interrupt the graph with the question from the AskHuman schema\n",
    "    location = interrupt(ask.question)\n",
    "    # Create a tool message once the user has responded with the location\n",
    "    tool_message = [{\"tool_call_id\": tool_call_id, \"type\": \"tool\", \"content\": location}]\n",
    "    return {\"messages\": tool_message}\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "workflow.add_node(\"ask_human\", ask_human)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "workflow.add_edge(START, \"agent\")\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "workflow.add_edge(\"ask_human\", \"agent\")\n",
    "\n",
    "# Set up memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "# Compile the workflow\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "# Draw the graph\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "11996216",
=======
   "id": "a02ce424",
>>>>>>> main
   "metadata": {},
   "source": [
    "Now, we ask the user where they are and look up the weather there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "9184cb08",
=======
   "id": "daff5c78",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [{\"role\": \"user\", \"content\": \"Ask the user where they are, then look up the weather there\"}]\n",
    "for event in app.stream({\"messages\": messages}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "7f61746d",
   "metadata": {
    "lines_to_next_cell": 0
   },
=======
   "id": "ba062e0c",
   "metadata": {},
>>>>>>> main
   "source": [
    "You can see that our graph got interrupted inside the `ask_human` node. It is now waiting for a location to be provided. You also notice that we use the [checkpointer](https://langchain-ai.github.io/langgraph/concepts/memory/#short-term-memory) to persist the state of the graph after the interrupt. This allows us to resume execution from the same state after the human has responded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "6b7f4f66",
=======
   "id": "648bd53e",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).next"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "d7a78e42",
=======
   "id": "0b1dfe1a",
>>>>>>> main
   "metadata": {},
   "source": [
    "### Using Command to Resume Execution\n",
    "\n",
    "After an interrupt, we need a way to continue execution. This is where the `Command` interface comes in. [The `Command` object has several powerful capabilities](https://langchain-ai.github.io/langgraph/how-tos/command/):\n",
    "- `resume`: Provides the value to return from the interrupt call\n",
    "- `goto`: Specifies which node to route to next\n",
    "- `update`: Modifies the state before continuing execution\n",
    "- `graph`: Controls navigation between parent and child graphs\n",
    "\n",
    "In this case, the `Command` object serves two crucial purposes:\n",
    "1. It provides the value to be returned from the `interrupt` call\n",
    "2. It controls the flow of execution in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "91c3f04e",
=======
   "id": "2276b2d6",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume execution with the value \"san francisco\"\n",
    "for event in app.stream(Command(resume=\"san francisco\"), config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "25b0fe5c",
=======
   "id": "4ae10bd5",
>>>>>>> main
   "metadata": {},
   "source": [
    "## Agent Inbox\n",
    "\n",
    "While we can implement basic human-in-the-loop functionality using `interrupt` calls and `Command` responses with the SDK, as shown above, this doesn't really scale well if we want to process a large number of emails and a variety of different actions. Our email assistant requires several types of human-in-the-loop interactions:\n",
    "\n",
    "1. **Email Triage Review**:\n",
    "   - When an email is classified as \"NOTIFY,\" humans should verify this classification\n",
    "   - Users should be able to accept the classification or provide feedback on how it should be classified\n",
    "\n",
    "2. **Email Response Review**:\n",
    "   - Before sending responses to important emails, humans should review the content\n",
    "   - Users need options to edit the draft, accept it as-is, provide feedback, or reject it entirely\n",
    "\n",
    "3. **Meeting Scheduling Review**:\n",
    "   - When the assistant proposes scheduling a meeting, humans should verify the details\n",
    "   - Users should be able to modify attendees, duration, date/time before accepting\n",
    "\n",
    "In general, any significant action (sending emails, scheduling meetings) requires human approval. Some low-risk tools (like calendar availability checks) can run without interruption. Of course, this is entirely dependent on the application and the risk of the action.\n",
    "\n",
    "### Agent Inbox: A Purpose-Built HITL Interface\n",
    "\n",
    "With this in mind, we built a simple interface for human-in-the-loop called [Agent Inbox](https://github.com/langchain-ai/agent-inbox) that allows us to review and approve or reject actions taken by LangGraph agents. Agent Inbox provides:\n",
    "\n",
    "1. **Structured Interaction Types**:\n",
    "   - `accept`: Approve the agent's action and continue\n",
    "   - `edit`: Modify the agent's proposed action before execution\n",
    "   - `response`: Provide feedback or answers without editing\n",
    "   - `ignore`: Reject the agent's action entirely\n",
    "\n",
    "2. **Rich Content Display**:\n",
    "   - Render (email) content in a readable format\n",
    "   - Support markdown for structured information presentation\n",
    "\n",
    "3. **Consistent User Experience**:\n",
    "   - Notification system for pending reviews\n",
    "   - Action buttons that match the allowed interaction types\n",
    "   - Thread-based organization of agent activities\n",
    "\n",
    "4. **Easy Integration with LangGraph**:\n",
    "   - Simple connection to local or hosted LangGraph deployments\n",
    "   - Compatible with LangGraph's interrupt mechanism\n",
    "   - No complex frontend development required\n",
    "\n",
    "### Integration with LangGraph's interrupt() Function\n",
    "\n",
    "Agent Inbox integrates seamlessly with LangGraph's `interrupt()` function. The integration works like this:\n",
    "\n",
    "1. **Request Structure**: We structure an interrupt request with specific fields:\n",
    "   ```python\n",
    "   request = {\n",
    "       \"action_request\": {\n",
    "           \"action\": \"write_email\",  # Name of the tool to call\n",
    "           \"args\": {\"to\": \"john@example.com\", \"subject\": \"Meeting\", \"content\": \"...\"}  # Action parameters\n",
    "       },\n",
    "       \"config\": {\n",
    "           \"allow_ignore\": True,   # Can dismiss the action\n",
    "           \"allow_respond\": True,  # Can provide feedback\n",
    "           \"allow_edit\": True,     # Can modify the action\n",
    "           \"allow_accept\": True,   # Can approve the action\n",
    "       },\n",
    "       \"description\": \"Email content to display...\" # Context shown to the user\n",
    "   }\n",
    "   ```\n",
    "\n",
    "2. **Passing to interrupt()**: We pass this request to the interrupt function:\n",
    "   ```python\n",
    "   response = interrupt([request])[0]  # Can batch multiple requests\n",
    "   ```\n",
    "\n",
    "3. **User Interaction**: Agent Inbox shows the request and collects the user's response\n",
    "\n",
    "4. **Handling Responses**: When execution resumes, we receive a structured `response`:\n",
    "   ```python\n",
    "   if response[\"type\"] == \"accept\":\n",
    "       # Execute the tool with original args\n",
    "   elif response[\"type\"] == \"edit\":\n",
    "       # Execute with edited args from response[\"args\"]\n",
    "   elif response[\"type\"] == \"ignore\":\n",
    "       # Skip execution\n",
    "   elif response[\"type\"] == \"response\":\n",
    "       # Process feedback from response[\"args\"]\n",
    "   ```\n",
    "\n",
    "This structured approach allows our email assistant to collect precise human input at critical decision points while maintaining a consistent user experience.\n",
    "\n",
    "## Email Assistant with Human-in-the-Loop\n",
    "\n",
    "Now that we understand both the interrupt mechanism and Agent Inbox, let's look at our email assistant implementation with human-in-the-loop. This implementation brings together all the concepts we've discussed:\n",
    "\n",
    "1. It uses the `interrupt` function to pause execution at key decision points\n",
    "2. It structures interrupt requests specifically for Agent Inbox\n",
    "3. It processes different response types from human reviewers\n",
    "4. It integrates these HITL capabilities into a full email processing workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "111921ba",
=======
   "id": "0aa1fca5",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "26810482",
=======
   "id": "5e82839a",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "from email_assistant.prompts import triage_system_prompt, triage_user_prompt, agent_system_prompt_hitl, default_background, default_triage_instructions, default_response_preferences, default_cal_preferences\n",
    "from email_assistant.schemas import State, RouterSchema, StateInput\n",
    "from email_assistant.utils import parse_email, format_for_display, format_email_markdown\n",
    "\n",
    "# Agent tools \n",
    "@tool\n",
    "def write_email(to: str, subject: str, content: str) -> str:\n",
    "    \"\"\"Write and send an email.\"\"\"\n",
    "    # Placeholder response - in real app would send email\n",
    "    return f\"Email sent to {to} with subject '{subject}' and content: {content}\"\n",
    "\n",
    "@tool\n",
    "def schedule_meeting(\n",
    "    attendees: list[str], subject: str, duration_minutes: int, preferred_day: str\n",
    ") -> str:\n",
    "    \"\"\"Schedule a calendar meeting.\"\"\"\n",
    "    # Placeholder response - in real app would check calendar and schedule\n",
    "    return f\"Meeting '{subject}' scheduled for {preferred_day} with {len(attendees)} attendees\"\n",
    "\n",
    "@tool\n",
    "def check_calendar_availability(day: str) -> str:\n",
    "    \"\"\"Check calendar availability for a given day.\"\"\"\n",
    "    # Placeholder response - in real app would check actual calendar\n",
    "    return f\"Available times on {day}: 9:00 AM, 2:00 PM, 4:00 PM\"\n",
    "\n",
    "@tool\n",
    "class Question(BaseModel):\n",
    "      \"\"\"Question to ask user.\"\"\"\n",
    "      content: str\n",
    "    \n",
    "@tool\n",
    "class Done(BaseModel):\n",
    "      \"\"\"E-mail has been sent.\"\"\"\n",
    "      done: bool\n",
    "\n",
    "# All tools available to the agent\n",
    "tools = [\n",
    "    write_email, \n",
    "    schedule_meeting, \n",
    "    check_calendar_availability, \n",
    "    Question, \n",
    "    Done,\n",
    "]\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "# Initialize the LLM for use with router / structured output\n",
    "llm = init_chat_model(\"openai:gpt-4o\", temperature=0.0)\n",
    "llm_router = llm.with_structured_output(RouterSchema) \n",
    "# Initialize the LLM, enforcing tool use (of any available tools) for agent\n",
    "llm = init_chat_model(\"openai:gpt-4o\", tool_choice=\"required\", temperature=0.0)\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "b4cc2a42",
=======
   "id": "a03acf00",
>>>>>>> main
   "metadata": {},
   "source": [
    "### Nodes of the Email Assistant\n",
    "\n",
    "Our email assistant has several key nodes that handle different aspects of the workflow:\n",
    "\n",
    "1. **triage_router**: This node is responsible for analyzing incoming emails and classifying them into three categories:\n",
    "   - **RESPOND**: Emails that require a response from the assistant.\n",
    "   - **NOTIFY**: Important emails that don't need a response but should be brought to the user's attention.\n",
    "   - **IGNORE**: Low-priority emails that can be safely ignored.\n",
    "   \n",
    "   The router uses a structured output LLM to make this classification and then routes to the appropriate next node based on its decision.\n",
    "\n",
    "2. **triage_interrupt_handler**: When an email is classified as \"notify,\" this handler creates an interrupt to display the email in Agent Inbox and collect human feedback. This allows users to:\n",
    "   - Confirm the notification classification\n",
    "   - Provide feedback on how they would prefer similar emails to be classified in the future\n",
    "   \n",
    "   This feedback loop is crucial for improving the assistant's classification over time.\n",
    "\n",
    "3. **llm_call**: This node invokes the LLM with the available tools to decide how to respond to an email. The LLM might decide to:\n",
    "   - Draft a response email\n",
    "   - Schedule a meeting\n",
    "   - Ask the user a question\n",
    "   - Or mark the email as done\n",
    "\n",
    "Each of these nodes plays a specific role in the overall email processing workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "b16e6ec8",
=======
   "id": "50f33320",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes \n",
    "def triage_router(state: State) -> Command[Literal[\"triage_interrupt_handler\", \"response_agent\", \"__end__\"]]:\n",
    "    \"\"\"Analyze email content to decide if we should respond, notify, or ignore.\n",
    "\n",
    "    The triage step prevents the assistant from wasting time on:\n",
    "    - Marketing emails and spam\n",
    "    - Company-wide announcements\n",
    "    - Messages meant for other teams\n",
    "    \"\"\"\n",
    "        \n",
    "    # Format system prompt with background and triage instructions\n",
    "    system_prompt = triage_system_prompt.format(\n",
    "        background=default_background,\n",
    "        triage_instructions=default_triage_instructions\n",
    "    )\n",
    "\n",
    "    # Parse the email input\n",
    "    author, to, subject, email_thread = parse_email(state[\"email_input\"])\n",
    "    user_prompt = triage_user_prompt.format(\n",
    "        author=author, to=to, subject=subject, email_thread=email_thread\n",
    "    )\n",
    "\n",
    "    # Create email markdown for Agent Inbox in case of notification  \n",
    "    email_markdown = format_email_markdown(subject, author, to, email_thread)\n",
    "\n",
    "    # Run the router LLM\n",
    "    result = llm_router.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Decision\n",
    "    classification = result.classification\n",
    "\n",
    "    # Process the classification decision\n",
    "    if classification == \"respond\":\n",
    "        print(\"ðŸ“§ Classification: RESPOND - This email requires a response\")\n",
    "        # Next node\n",
    "        goto = \"response_agent\"\n",
    "        # Update the state\n",
    "        update = {\n",
    "            \"classification_decision\": result.classification,\n",
    "            \"messages\": [{\"role\": \"user\",\n",
    "                            \"content\": f\"Respond to the email: {email_markdown}\"\n",
    "                        }],\n",
    "        }\n",
    "    elif classification == \"ignore\":\n",
    "        print(\"ðŸš« Classification: IGNORE - This email can be safely ignored\")\n",
    "\n",
    "        # Next node\n",
    "        goto = END\n",
    "        # Update the state\n",
    "        update = {\n",
    "            \"classification_decision\": classification,\n",
    "        }\n",
    "\n",
    "    elif classification == \"notify\":\n",
    "        print(\"ðŸ”” Classification: NOTIFY - This email contains important information\") \n",
    "\n",
    "        # Next node\n",
    "        goto = \"triage_interrupt_handler\"\n",
    "        # Update the state\n",
    "        update = {\n",
    "            \"classification_decision\": classification,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid classification: {classification}\")\n",
    "    return Command(goto=goto, update=update)\n",
    "\n",
    "def triage_interrupt_handler(state: State) -> Command[Literal[\"response_agent\", \"__end__\"]]:\n",
    "    \"\"\"Handles interrupts from the triage step\"\"\"\n",
    "    \n",
    "    # Parse the email input\n",
    "    author, to, subject, email_thread = parse_email(state[\"email_input\"])\n",
    "\n",
    "    # Create email markdown for Agent Inbox in case of notification  \n",
    "    email_markdown = format_email_markdown(subject, author, to, email_thread)\n",
    "\n",
    "    # Create messages to save to memory\n",
    "    messages = [{\"role\": \"user\",\n",
    "                \"content\": f\"Classification Decision: {state['classification_decision']} for email: {email_markdown}\"\n",
    "                }]\n",
    "\n",
    "    # Create interrupt for Agent Inbox\n",
    "    request = {\n",
    "        \"action_request\": {\n",
    "            \"action\": f\"Email Assistant: {state['classification_decision']}\",\n",
    "            \"args\": {}\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"allow_ignore\": True,  \n",
    "            \"allow_respond\": True, # Allow user feedback if decision is not correct \n",
    "            \"allow_edit\": False, \n",
    "            \"allow_accept\": False,  \n",
    "        },\n",
    "        # Email to show in Agent Inbox\n",
    "        \"description\": email_markdown,\n",
    "    }\n",
    "\n",
    "    # Agent Inbox responds with a list  \n",
    "    response = interrupt([request])[0]\n",
    "\n",
    "    # Accept the decision and end   \n",
    "    if response[\"type\"] == \"accept\":\n",
    "        goto = END \n",
    "\n",
    "    # If user provides feedback, update memory  \n",
    "    elif response[\"type\"] == \"response\":\n",
    "        # Add feedback to messages \n",
    "        user_input = response[\"args\"]\n",
    "        messages.append({\"role\": \"user\",\n",
    "                        \"content\": f\"Here is feedback on how the user would prefer the email to be classified: {user_input}\"\n",
    "                        })\n",
    "\n",
    "        goto = END\n",
    "\n",
    "    # Update the state \n",
    "    update = {\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "    return Command(goto=goto, update=update)\n",
    "\n",
    "def llm_call(state: State):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": agent_system_prompt_hitl.format(background=default_background,\n",
    "                                                                                  response_preferences=default_response_preferences, \n",
    "                                                                                  cal_preferences=default_cal_preferences)}\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "e51f58d1",
=======
   "id": "ca358fab",
>>>>>>> main
   "metadata": {},
   "source": [
    "### The interrupt_handler\n",
    "\n",
    "The `interrupt_handler` is the core HITL component of our response agent. Its job is to examine the tool calls that the LLM wants to make and determine which ones need human review before execution. Here's how it works:\n",
    "\n",
    "1. **Tool Selection**: The handler maintains a list of \"HITL tools\" that require human approval:\n",
    "   - `write_email`: Since sending emails has significant external impact\n",
    "   - `schedule_meeting`: Since scheduling meetings affects calendars\n",
    "   - `Question`: Since asking users questions requires direct interaction\n",
    "\n",
    "2. **Direct Execution**: Tools not in the HITL list (like `check_calendar_availability`) are executed immediately without interruption. This allows low-risk operations to proceed automatically.\n",
    "\n",
    "3. **Context Preparation**: For tools requiring review, the handler:\n",
    "   - Retrieves the original email for context\n",
    "   - Formats the tool call details for clear display\n",
    "   - Configures which interaction types are allowed for each tool type\n",
    "\n",
    "4. **Interrupt Creation**: The handler creates a structured interrupt request with:\n",
    "   - The action name and arguments\n",
    "   - Configuration for allowed interaction types\n",
    "   - A description that includes both the original email and the proposed action\n",
    "\n",
    "5. **Response Processing**: After the interrupt, the handler processes the human response:\n",
    "   - **Accept**: Executes the tool with original arguments\n",
    "   - **Edit**: Updates the tool call with edited arguments and then executes\n",
    "   - **Ignore**: Cancels the tool execution\n",
    "   - **Response**: Records feedback without execution\n",
    "\n",
    "This handler ensures humans have oversight of all significant actions while allowing routine operations to proceed automatically. The ability to edit tool arguments (like email content or meeting details) gives users precise control over the assistant's actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "86b2f669",
=======
   "id": "b214072f",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "def interrupt_handler(state: State):\n",
    "    \"\"\"Creates an interrupt for human review of tool calls\"\"\"\n",
    "    \n",
    "    # Store messages\n",
    "    result = []\n",
    "\n",
    "    # Iterate over the tool calls in the last message\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        \n",
    "        # Tools that require human review\n",
    "        hitl_tools = [\"write_email\", \"schedule_meeting\", \"Question\"]\n",
    "        \n",
    "        # If tool is not in our HITL list, execute it directly without interruption\n",
    "        if tool_call[\"name\"] not in hitl_tools:\n",
    "            # Execute search_memory and other tools without interruption\n",
    "            tool = tools_by_name[tool_call[\"name\"]]\n",
    "            observation = tool.invoke(tool_call[\"args\"])\n",
    "            result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": tool_call[\"id\"]})\n",
    "            continue\n",
    "            \n",
    "        # Get original email from email_input in state\n",
    "        original_email_markdown = \"\"\n",
    "        if \"email_input\" in state:\n",
    "            email_input = state[\"email_input\"]\n",
    "            author, to, subject, email_thread = parse_email(email_input)\n",
    "            original_email_markdown = format_email_markdown(subject, author, to, email_thread)\n",
    "        \n",
    "        # Format tool call for display and prepend the original email\n",
    "        tool_display = format_for_display(state, tool_call)\n",
    "        description = original_email_markdown + tool_display\n",
    "\n",
    "        # Configure what actions are allowed in Agent Inbox\n",
    "        if tool_call[\"name\"] == \"write_email\":\n",
    "            config = {\n",
    "                \"allow_ignore\": True,\n",
    "                \"allow_respond\": True,\n",
    "                \"allow_edit\": True,\n",
    "                \"allow_accept\": True,\n",
    "            }\n",
    "        elif tool_call[\"name\"] == \"schedule_meeting\":\n",
    "            config = {\n",
    "                \"allow_ignore\": True,\n",
    "                \"allow_respond\": True,\n",
    "                \"allow_edit\": True,\n",
    "                \"allow_accept\": True,\n",
    "            }\n",
    "        elif tool_call[\"name\"] == \"Question\":\n",
    "            config = {\n",
    "                \"allow_ignore\": True,\n",
    "                \"allow_respond\": True,\n",
    "                \"allow_edit\": False,\n",
    "                \"allow_accept\": False,\n",
    "            }\n",
    "\n",
    "        # Create the interrupt request\n",
    "        request = {\n",
    "            \"action_request\": {\n",
    "                \"action\": tool_call[\"name\"],\n",
    "                \"args\": tool_call[\"args\"]\n",
    "            },\n",
    "            \"config\": config,\n",
    "            \"description\": description,\n",
    "        }\n",
    "\n",
    "        # Send to Agent Inbox and wait for response\n",
    "        response = interrupt([request])[0]\n",
    "\n",
    "        # Handle the responses \n",
    "        if response[\"type\"] == \"accept\":\n",
    "\n",
    "            # Execute the tool with original args\n",
    "            tool = tools_by_name[tool_call[\"name\"]]\n",
    "            observation = tool.invoke(tool_call[\"args\"])\n",
    "            result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": tool_call[\"id\"]})\n",
    "                        \n",
    "        elif response[\"type\"] == \"edit\":\n",
    "\n",
    "            # Tool selection \n",
    "            tool = tools_by_name[tool_call[\"name\"]]\n",
    "            \n",
    "            # Get edited args from Agent Inbox\n",
    "            edited_args = response[\"args\"][\"args\"]\n",
    "\n",
    "            # Save feedback in memory and update the write_email tool call with the edited content from Agent Inbox\n",
    "            if tool_call[\"name\"] == \"write_email\":\n",
    "                \n",
    "                # Update the AI message's tool call with edited content (reference to the message in the state)\n",
    "                ai_message = state[\"messages\"][-1]\n",
    "                current_id = tool_call[\"id\"]\n",
    "                \n",
    "                # Replace the original tool call with the edited one (any changes made to this reference affect the original object in the state)\n",
    "                ai_message.tool_calls = [tc for tc in ai_message.tool_calls if tc[\"id\"] != current_id] + [\n",
    "                    {\"type\": \"tool_call\", \"name\": tool_call[\"name\"], \"args\": edited_args, \"id\": current_id}\n",
    "                ]\n",
    "                \n",
    "                # Execute the tool with edited args\n",
    "                observation = tool.invoke(edited_args)\n",
    "                \n",
    "                # Add only the tool response message\n",
    "                result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": current_id})\n",
    "            \n",
    "            # Save feedback in memory and update the schedule_meeting tool call with the edited content from Agent Inbox\n",
    "            elif tool_call[\"name\"] == \"schedule_meeting\":\n",
    "                \n",
    "                # Update the AI message's tool call with edited content\n",
    "                ai_message = state[\"messages\"][-1]\n",
    "                current_id = tool_call[\"id\"]\n",
    "                \n",
    "                # Replace the original tool call with the edited one\n",
    "                ai_message.tool_calls = [tc for tc in ai_message.tool_calls if tc[\"id\"] != current_id] + [\n",
    "                    {\"type\": \"tool_call\", \"name\": tool_call[\"name\"], \"args\": edited_args, \"id\": current_id}\n",
    "                ]\n",
    "                \n",
    "                # Execute the tool with edited args\n",
    "                observation = tool.invoke(edited_args)\n",
    "                \n",
    "                # Add only the tool response message\n",
    "                result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": current_id})\n",
    "\n",
    "        elif response[\"type\"] == \"ignore\":\n",
    "            # Don't execute the tool\n",
    "            result.append({\"role\": \"tool\", \"content\": \"Tool execution cancelled by user\", \"tool_call_id\": tool_call[\"id\"]})\n",
    "            \n",
    "        elif response[\"type\"] == \"response\":\n",
    "            # User provided feedback\n",
    "            user_feedback = response[\"args\"]\n",
    "            result.append({\"role\": \"tool\", \"content\": f\"Feedback: {user_feedback}\", \"tool_call_id\": tool_call[\"id\"]})\n",
    "            \n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "3c423948",
=======
   "id": "aa497017",
>>>>>>> main
   "metadata": {},
   "source": [
    "### HITL Email Assistant Workflow\n",
    "\n",
    "Now we can integrate everything into a complete workflow that connects all the components. The workflow consists of two main parts:\n",
    "\n",
    "1. **Response Agent Subgraph**:\n",
    "   First, we build a standalone agent that can handle email responses:\n",
    "   - The `llm_call` node generates responses or tool calls\n",
    "   - The `should_continue` function checks if the agent is done or needs to use a tool\n",
    "   - The `interrupt_handler` manages human review of tool execution\n",
    "   - The cycle continues until the agent reaches a conclusion\n",
    "   \n",
    "   This response agent is compiled as a reusable subgraph.\n",
    "\n",
    "2. **Overall Email Assistant Workflow**:\n",
    "   Then, we create the main workflow that:\n",
    "   - Starts with `triage_router` to classify the email\n",
    "   - Routes to `triage_interrupt_handler` for NOTIFY classifications\n",
    "   - Routes to `response_agent` for RESPOND classifications\n",
    "   - Ends immediately for IGNORE classifications\n",
    "\n",
    "This architecture provides a clean separation of concerns, with distinct components for triage, response generation, and human oversight. The resulting workflow gives us a complete email assistant that:\n",
    "\n",
    "- Analyzes incoming emails\n",
    "- Correctly routes them based on importance\n",
    "- Engages humans for oversight at critical decision points\n",
    "- Responds appropriately to important emails\n",
    "\n",
    "The final graph visualization shows the complete flow from email input through triage and, when necessary, through the response generation process with human oversight at each significant step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "c34c8497",
=======
   "id": "f32317df",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conditional edge function\n",
    "def should_continue(state: State) -> Literal[\"interrupt_handler\", END]:\n",
    "    \"\"\"Route to tool handler, or end if Done tool called\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls: \n",
    "            if tool_call[\"name\"] == \"Done\":\n",
    "                return END\n",
    "            else:\n",
    "                return \"interrupt_handler\"\n",
    "\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"interrupt_handler\", interrupt_handler)\n",
    "\n",
    "# Add edges\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"interrupt_handler\": \"interrupt_handler\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"interrupt_handler\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "response_agent = agent_builder.compile()\n",
    "\n",
    "# Build overall workflow\n",
    "overall_workflow = (\n",
    "    StateGraph(State, input=StateInput)\n",
    "    .add_node(triage_router)\n",
    "    .add_node(triage_interrupt_handler)\n",
    "    .add_node(\"response_agent\", response_agent)\n",
    "    .add_edge(START, \"triage_router\")\n",
    "    \n",
    ")\n",
    "\n",
    "email_assistant = overall_workflow.compile()\n",
    "display(Image(email_assistant.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "d90043a3",
=======
   "id": "5668acf2",
>>>>>>> main
   "metadata": {},
   "source": [
    "### Providing Feedback on Email Responses\n",
    "\n",
    "Let's see the HITL workflow in action with a practical example. Here we'll demonstrate how a user can provide feedback on the assistant's proposed actions. This example shows how to:\n",
    "\n",
    "1. Process an incoming email about scheduling a tax planning call\n",
    "2. Review the assistant's proposed action in Agent Inbox\n",
    "3. Provide feedback without changing the proposal\n",
    "4. Complete the workflow with the feedback incorporated\n",
    "\n",
    "This demonstration shows the **response** interaction type in Agent Inbox, which allows users to provide guidance without directly editing the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "63f0ae09",
=======
   "id": "d5a34f7c",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from src.email_assistant.email_assistant_hitl import overall_workflow\n",
    "\n",
    "# Respond\n",
    "email_input =  {\n",
    "    \"to\": \"Lance Martin <lance@company.com>\",\n",
    "    \"author\": \"Project Manager <pm@client.com>\",\n",
    "    \"subject\": \"Tax season let's schedule call\",\n",
    "    \"email_thread\": \"Lance,\\n\\nIt's tax season again, and I wanted to schedule a call to discuss your tax planning strategies for this year. I have some suggestions that could potentially save you money.\\n\\nAre you available sometime next week? Tuesday or Thursday afternoon would work best for me, for about 45 minutes.\\n\\nRegards,\\nProject Manager\"\n",
    "}\n",
    "\n",
    "# Compile the graph\n",
    "checkpointer = MemorySaver()\n",
    "graph = overall_workflow.compile(checkpointer=checkpointer)\n",
    "thread_config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "# Run the graph until the first interrupt\n",
    "for chunk in graph.stream({\"email_input\": email_input}, config=thread_config):\n",
    "   print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "50b01434",
=======
   "id": "c9976831",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "# First, we respond with feedback about meeting duration\n",
    "# This sends a \"response\" type interaction from Agent Inbox\n",
    "# The feedback suggests shorter meetings (30 mins instead of 45 mins)\n",
    "for chunk in graph.stream(Command(resume=[{\"type\": \"response\", \n",
    "                                          \"args\": \"Let's suggest 30 minute calls in the future!'\"}]), config=thread_config):\n",
    "   print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "cf9e1002",
=======
   "id": "1a3bde58",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we can examine the interrupt object to understand what action was proposed\n",
    "# This shows the details of the action the agent wanted to take\n",
    "Interrupt_Object = chunk['__interrupt__'][0]\n",
    "Interrupt_Object.value[0]['action_request']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "87952241",
=======
   "id": "b6a243f6",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "# Now we accept the tool execution (likely to schedule a meeting)\n",
    "# This simulates clicking the \"Accept\" button in Agent Inbox\n",
    "# The original action will proceed as planned, but our feedback is recorded\n",
    "for chunk in graph.stream(Command(resume=[{\"type\": \"accept\", \n",
    "                                          \"args\": \"\"}]), config=thread_config):\n",
    "   print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "96e6b530",
=======
   "id": "8547f901",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "state = graph.get_state(thread_config)\n",
    "for m in state.values['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "2c20a4af",
=======
   "id": "c3a32b61",
>>>>>>> main
   "metadata": {},
   "source": [
    "### Editing Proposed Actions\n",
    "\n",
    "Now let's look at a more direct form of human intervention: editing the assistant's proposed actions before they're executed. This example demonstrates how to:\n",
    "\n",
    "1. Process the same email about tax planning\n",
    "2. Review the assistant's proposed meeting scheduling in Agent Inbox\n",
    "3. Edit the meeting details (attendees, subject, duration, and date)\n",
    "4. Accept the edited action for execution\n",
    "\n",
    "This demonstrates the **edit** interaction type in Agent Inbox, which allows users to modify the details of an action before approving it. This is particularly useful for:\n",
    "- Correcting meeting parameters\n",
    "- Adjusting email content before sending\n",
    "- Fine-tuning any action that requires precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "02552c01",
=======
   "id": "9049ce29",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph\n",
    "checkpointer = MemorySaver()\n",
    "graph = overall_workflow.compile(checkpointer=checkpointer)\n",
    "thread_config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "# Run the graph until the first interrupt\n",
    "for chunk in graph.stream({\"email_input\": email_input}, config=thread_config):\n",
    "   print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "a868a828",
=======
   "id": "12d1627f",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we edit the meeting details before they're confirmed\n",
    "# This simulates using the edit feature in Agent Inbox to modify:\n",
    "# 1. The attendee list (ensuring both parties are included)\n",
    "# 2. The meeting subject (making it more descriptive)\n",
    "# 3. The duration (reducing from 45 to 30 minutes)\n",
    "# 4. The specific date (setting an exact day rather than a general preference)\n",
    "for chunk in graph.stream(Command(resume=[{\"type\": \"edit\",  \n",
    "                                           \"args\": {\"args\": {\"attendees\": ['pm@client.com', 'lance@company.com'],\n",
    "                                                             \"subject\": \"Tax Planning Strategies Discussion\",\n",
    "                                                             \"duration_minutes\": 30,\n",
    "                                                             'preferred_day': '2023-11-07'}\n",
    "                                                             }\n",
    "                                                             }]), config=thread_config):\n",
    "   print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "3a22c40c",
=======
   "id": "126ec0f8",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "# After editing, we need to accept the final action\n",
    "# This simulates clicking \"Accept\" in Agent Inbox after making edits\n",
    "# The tool will execute with our edited parameters instead of the original ones\n",
    "for chunk in graph.stream(Command(resume=[{\"type\": \"accept\", \n",
    "                                          \"args\": \"\"}]), config=thread_config):\n",
    "   print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "321d1556",
=======
   "id": "f718043b",
>>>>>>> main
   "metadata": {},
   "source": [
    "## Testing with Local Deployment\n",
    "\n",
    "You can find this graph in the `src/email_assistant` directory:\n",
    "\n",
    "* `src/email_assistant/email_assistant_hitl.py`\n",
    "\n",
    "You can test it locally in LangGraph Studio by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "0e1fd2bb",
=======
   "id": "cb178828",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "! langgraph dev"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "9e10412d",
=======
   "id": "923b2185",
>>>>>>> main
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Example e-mail you can test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "4b0a48ec",
=======
   "id": "1797928e",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "email_input = {\n",
    "  \"author\": \"Alice Smith <alice.smith@company.com>\",\n",
    "  \"to\": \"John Doe <john.doe@company.com>\",\n",
    "  \"subject\": \"Quick question about API documentation\",\n",
    "  \"email_thread\": \"Hi John,\\nI was reviewing the API documentation for the new authentication service and noticed a few endpoints seem to be missing from the specs. Could you help clarify if this was intentional or if we should update the docs?\\nSpecifically, I'm looking at:\\n- /auth/refresh\\n- /auth/validate\\nThanks!\\nAlice\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "f4768f28",
=======
   "id": "a2b8ecea",
>>>>>>> main
   "metadata": {},
   "source": [
    "When you submit the e-mail, you will see the interrupt in Studio. This displays the full `request` object that was passed to the `interrupt` function.\n",
    "\n",
    "![studio-img](img/studio-interrupt.png)\n",
    " \n",
    "If you go to [dev.agentinbox.ai](https://dev.agentinbox.ai/), you can add the graph url:\n",
    "   * Graph name: the name from the `langgraph.json` file (`email_assistant_hitl`)\n",
    "   * Graph URL: `http://127.0.0.1:2024/`\n",
    "\n",
    "All interrupted threads run will be visible. Any e-mail triaged to `Notify` will be displayed in Agent Inbox with the action request in `triage_interrupt_handler`, allowing you to ignore or respond with feedback. In addition, any email marked with `Respond` will be displayed in Agent Inbox with the action request in `interrupt_handler`, allowing you to review tool calls from `[\"write_email\", \"schedule_meeting\", \"Question\"]`.\n",
    "\n",
    "![agent-inbox-img](img/agent-inbox.png)\n",
    "\n",
    "In the case of `write_email`, you can fully edit the e-mail content, accept it as-is, ignore it, or provide feedback as specified in the `config` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "id": "beac0eb0",
=======
   "id": "691064b0",
>>>>>>> main
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"allow_ignore\": True,\n",
    "    \"allow_respond\": True,\n",
    "    \"allow_edit\": True,\n",
    "    \"allow_accept\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "cd866bd0",
=======
   "id": "c34670ab",
>>>>>>> main
   "metadata": {},
   "source": [
    "![agent-inbox-img](img/agent-inbox-draft.png)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
