{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e21aa1",
   "metadata": {},
   "source": [
    "# Evaluating our Agent\n",
    "\n",
    "We've written an agentic workflow that uses a router to triage the email and then passes the email to the agent for response generation. How can we be sure that it will work well in production? This is why testing is important: it guides our decisions about our agent architecture with quantifiable metrics like response quality, token usage, latency, or triage accuracy. [LangSmith](https://docs.smith.langchain.com/) offers two primary ways to test agents. \n",
    "\n",
    "![overview-img](img/overview_eval.png)\n",
    "\n",
    "## Test Approaches \n",
    "\n",
    "### Pytest\n",
    "\n",
    "[Pytest](https://docs.pytest.org/en/stable/) is well known to many developers as a powerful tool for writing tests within the Python ecosystem. LangSmith integrates with pytest to allow you to write tests that we can run on each assistant and log the results to LangSmith. Pytest is a great way to get started quickly with a framework you're already familiar with.\n",
    "\n",
    "### LangSmith Datasets \n",
    "\n",
    "You can also create a dataset [in LangSmith](https://docs.smith.langchain.com/evaluation) and run each assistant against the dataset using the LangSmith evaluate API. LangSmith datasets are great for teams who are collaboratively building out their test suite. You can leverage production traces, annotation queues, and more, to add examples to an ever-growing golden dataset.\n",
    "\n",
    "## Test Cases\n",
    "\n",
    "Testing often starts with defining the test cases, which can be a challenging process. In this case, we'll just define a set of example emails we want to handle along with a few things to test. You can see the test cases in `eval/email_dataset.py`, which contains the following:\n",
    "\n",
    "1. **Input Emails**: A collection of diverse email examples\n",
    "2. **Ground Truth Classifications**: `Respond`, `Notify`, `Ignore`\n",
    "3. **Expected Tool Calls**: Tools called for each email that requires a response\n",
    "4. **Response Criteria**: What makes a good response for emails requiring replies\n",
    "\n",
    "## Pytest Example\n",
    "\n",
    "Here's a simple example of testing using Pytest. \n",
    "\n",
    "We will test whether our `email_assistant` makes the right tool calls when responding to the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9005acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from eval.email_dataset import email_inputs, expected_tool_calls\n",
    "from email_assistant.utils import format_messages_string\n",
    "from email_assistant.email_assistant import email_assistant\n",
    "from email_assistant.utils import extract_tool_calls\n",
    "\n",
    "from langsmith import testing as t\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "@pytest.mark.langsmith\n",
    "@pytest.mark.parametrize(\n",
    "    \"email_input, expected_calls\",\n",
    "    [   # Pick some examples with e-mail reply expected\n",
    "        (email_inputs[0],expected_tool_calls[0]),\n",
    "        (email_inputs[3],expected_tool_calls[3]),\n",
    "    ],\n",
    ")\n",
    "def test_email_dataset_tool_calls(email_input, expected_calls):\n",
    "    \"\"\"Test if email processing contains expected tool calls.\"\"\"\n",
    "    # Run the email assistant\n",
    "    messages = [{\"role\": \"user\", \"content\": str(email_input)}]\n",
    "    result = email_assistant.invoke({\"messages\": messages})\n",
    "            \n",
    "    # Extract tool calls from messages list\n",
    "    extracted_tool_calls = extract_tool_calls(result['messages'])\n",
    "            \n",
    "    # Check if all expected tool calls are in the extracted ones\n",
    "    missing_calls = [call for call in expected_calls if call.lower() not in extracted_tool_calls]\n",
    "    \n",
    "    t.log_outputs({\n",
    "                \"missing_calls\": missing_calls,\n",
    "                \"extracted_tool_calls\": extracted_tool_calls,\n",
    "                \"response\": format_messages_string(result['messages'])\n",
    "            })\n",
    "\n",
    "    # Test passes if no expected calls are missing\n",
    "    assert len(missing_calls) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700aba2a",
   "metadata": {},
   "source": [
    "You'll notice a few things. First, to [run with Pytest and log test results to LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest), we only need to add the `@pytest.mark.langsmith ` decorator to our function and place it in a file, as you see in `notebooks/test_tools.py`. Second, we can pass dataset examples to the test function as shown [here](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#parametrize-with-pytestmarkparametrize) via `@pytest.mark.parametrize`. We can run the test from the command line. From the project root, run:\n",
    "\n",
    "```\n",
    "! LANGSMITH_TEST_SUITE='Email assistant: Test Tools'  pytest notebooks/test_tools.py\n",
    "```\n",
    "\n",
    "We can view the results in the LangSmith UI. The `assert len(missing_calls) == 0` is logged to the `Pass` column in LangSmith. The `log_outputs` are passed to the `Outputs` column and function arguments are passed to the `Inputs` column. Each input passed in `@pytest.mark.parametrize(` is a separate row logged to the `LANGSMITH_TEST_SUITE` project name in LangSmith, which is found under `Datasets & Experiments`.\n",
    "\n",
    "![Test Results](img/test_result.png)\n",
    "\n",
    "## LangSmith Datasets \n",
    "\n",
    "### Dataset Definition \n",
    "\n",
    "In addition to the Pytest approach, we can also [create a dataset in LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset) with the LangSmith SDK. This creates a dataset with the test cases in the `eval/email_dataset.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ea997ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from eval.email_dataset import examples_triage\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = \"Interrupt Workshop: E-mail Triage Dataset\"\n",
    "\n",
    "# Create dataset if it doesn't exist\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, \n",
    "        description=\"A dataset of e-mails and their triage decisions.\"\n",
    "    )\n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(dataset_id=dataset.id, examples=examples_triage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2df606",
   "metadata": {},
   "source": [
    "### Run Agents \n",
    "\n",
    "The dataset has the following structure, with an e-mail input and a ground truth classification for the e-mail as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This is just an example, this cell won't run\n",
    "examples_triage = [\n",
    "  {\n",
    "      \"inputs\": {\"email_input\": email_input_1},\n",
    "      \"outputs\": {\"classification\": triage_output_1},\n",
    "  }, ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290e820",
   "metadata": {},
   "source": [
    "We define functions that take dataset inputs and pass them to each agent we want to evaluate. The function just takes the `inputs` dict from the dataset and passes it to the agent. It returns a dict with the agent's output. Here, we specifically look to evaluate the `email_assistant`'s triage classification decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b9d1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_email_assistant(inputs: dict) -> dict:\n",
    "    \"\"\"Process an email through the workflow-based email assistant.\"\"\"\n",
    "    response = email_assistant.invoke({\"email_input\": inputs[\"email_input\"]})\n",
    "    return {\"classification_decision\": response['classification_decision']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6ec4c",
   "metadata": {},
   "source": [
    "The LangSmith [evaluate API](https://docs.smith.langchain.com/evaluation) passes the `inputs` dict to this function. \n",
    "\n",
    "### Evaluator Function \n",
    "\n",
    "We also create an evaluator function. What do we want to evaluate? We have reference outputs in our dataset and agent outputs defined in the functions above.\n",
    "\n",
    "* Reference outputs: `\"outputs\": {\"classification\": triage_output_1} ...`\n",
    "* Agent outputs: `\"outputs\": {\"classification_decision\": agent_output_1} ...`\n",
    "\n",
    "We want to evaluate if the agent's output matches the reference output. So we simply need a an evaluator function that compares the two, where `outputs` is the agent's output and `reference_outputs` is the reference output from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fee7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_evaluator(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n",
    "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd2de9",
   "metadata": {},
   "source": [
    "### Running Evaluation\n",
    "\n",
    "Now, the question is: how are these things hooked together? The evaluate API takes care of it for us. It passes the `inputs` dict from our dataset the target function. It passes the `outputs` dict from our dataset to the evaluator function. And it passes the output of our agent to the evaluator function. Note this is similar to what we did with Pytest: in Pytest, we passed in the dataset example inputs and reference outputs to the test function with `@pytest.mark.parametrize`.\n",
    "\n",
    "![overview-img](img/eval_detail.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6807306d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'E-mail assistant workflow-92f462c6' at:\n",
      "https://smith.langchain.com/o/ebbaf2eb-769b-4505-aca2-d11de10372a4/datasets/cdd3f95a-aaf7-4b19-8fc4-74bc5a7df870/compare?selectedSessions=909b3c47-7454-4b5d-9637-151f1dc6b674\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e76a7c362a54d71b2d475931d14c3c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸ”” Classification: NOTIFY - This email contains important information\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸ”” Classification: NOTIFY - This email contains important information\n",
      "ðŸ”” Classification: NOTIFY - This email contains important information\n",
      "ðŸ”” Classification: NOTIFY - This email contains important information\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸš« Classification: IGNORE - This email can be safely ignored\n",
      "ðŸ”” Classification: NOTIFY - This email contains important information\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸ“§ Classification: RESPOND - This email requires a response\n",
      "ðŸš« Classification: IGNORE - This email can be safely ignored\n"
     ]
    }
   ],
   "source": [
    "experiment_results_workflow = client.evaluate(\n",
    "    # Run agent \n",
    "    target_email_assistant,\n",
    "    # Dataset name   \n",
    "    data=dataset_name,\n",
    "    # Evaluator\n",
    "    evaluators=[classification_evaluator],\n",
    "    # Name of the experiment\n",
    "    experiment_prefix=\"E-mail assistant workflow\", \n",
    "    # Number of concurrent evaluations\n",
    "    max_concurrency=2, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76baff88",
   "metadata": {},
   "source": [
    "We can view the results from both experiments in the LangSmith UI.\n",
    "\n",
    "![Test Results](img/eval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b44111d",
   "metadata": {},
   "source": [
    "### Running against a Larger Test Suite\n",
    "Now that we've seen how to evaluate our agents using Pytest and evaluate(), we can use evaluations over a larger test suite to get a better sense of how our agent performs over a larger set of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d3747c",
   "metadata": {},
   "source": [
    "Let's run our `email_assistant` against a larger test suite.\n",
    "```\n",
    "! LANGSMITH_TEST_SUITE='Email assistant: Test Full Response' LANGSMITH_EXPERIMENT='email_assistant' pytest tests/test_response.py --agent-module email_assistant\n",
    "```\n",
    "\n",
    "Now let's take a look at this experiment in the LangSmith UI and look into what our agent did well, and what it could improve on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca836fbf",
   "metadata": {},
   "source": [
    "### Getting Results\n",
    "\n",
    "We can also get the results of the evaluation, by reading our experiment projects This is great if we want to create our own visualizations on our agent performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70b655f8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency p50: 0:00:08.390000\n",
      "Latency p99: 0:00:19.439500\n",
      "Token Usage: 76879\n",
      "Feedback Stats: {'pass': {'n': 16, 'avg': 0.9375, 'stdev': 0.24206145913796356, 'errors': 0, 'values': {}}}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Copy your experiment name here\n",
    "experiment_name = \"email_assistant:3c9967e4\"\n",
    "email_assistant_experiment_results = client.read_project(project_name=experiment_name, include_stats=True)\n",
    "\n",
    "print(\"Latency p50:\", email_assistant_experiment_results.latency_p50)\n",
    "print(\"Latency p99:\", email_assistant_experiment_results.latency_p99)\n",
    "print(\"Token Usage:\", email_assistant_experiment_results.total_tokens)\n",
    "print(\"Feedback Stats:\", email_assistant_experiment_results.feedback_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2095be27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
